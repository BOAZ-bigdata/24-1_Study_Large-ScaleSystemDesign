# 8~9장

## 8장 URL 단축기 기능 설계

tinyurl같은 URL 단축기는 고전적인 시스템 설계 문제 중 하나이다.

이 시스템의 기본적인 기능은 다음과 같다.

- URL 단축: 주어진 긴 URL을 훨씬 짧게 줄인다.
- URL 리디렉션(redirection): 축약된 URL로 HTTP 요청이 오면 원래 URL로 redirection한다.
- 높은 가용성과 규모 확장성, 그리고 장애 감내가 요구됨.

### API 리디렉션(redirection)

흔히 리디렉션을 위해 브라우저에 HTTP 301 상태 코드와 Location 헤더에 리디렉션할 URL을 응답한다.

**여기서 301 대신에 302도 활용할 수 있는데, 이 둘의 차이는 다음과 같고 적절한 상황에 선택하여 활용하자.**

- `301 Permanently Moved`
  - 이 응답은 영구적으로 URL이 이전되었다는 응답이다. 따라서 Location 헤더로 반환한 URL을 브라우저는 캐시(cache)하게 된다.
  - 따라서 서버의 부하를 줄일 수 있다.
- `302 Found`
  - 이 응답은 301과 달리 `일시적`으로 URL이 이전되었다는 응답이다. 따라서 301과 달리 브라우저는 URL을 캐시하지 않는다.
  - 이렇게하면 매번 URl 단축기에 브라우저가 요청을 보내므로 트래픽 분석 등 데이터 분석에 활용해볼 수 있다.

### URl 단축

단축 URL을 만드는 방법은 해시 함수를 이용한다.

결국 중요한 것은 긴 URL을 해시 값으로 대응시킬 수 있는 함수 fx를 찾는 것이다.

해시 함수는 다음 요구사항을 만족해야한다.

- 입력으로 주어진 긴 URL이 다른 값이면 해시 값도 달라야 함
- 계산된 해시 값은 원래 입력으로 주어졌던 긴 URL로 복원될 수 있어야함

### 해시 함수

URL 단축기에서는 해시 함수를 적절히 설정하는 것이 중요하다.

해시 함수가 계산한 단축 URL 값을 `hashValue`라고 칭하면,

`hashValue`는 [0-9, a-z, A-Z]의 문자들로 구성될 수 있을 것이다.
이는 10 + 26 + 26 = 총 62개의 문자를 사용할 수 있다는 걸 의미하고 따라서 hashValue의 길이에 따라 62^n개의 URL을 수용할 수 있게된다. 여기서 n의 값은 시스템 요구사항에 맞게 결정된다.

이 글에서는 n을 7로 잡고 약 3.5조 개의 URL을 수용할 수 있는 시스템 기준으로 얘기할 것이다.

이어서 해시 함수 구현에 쓰일 기술인 `해시 후 충돌 해소` 와 `base-62 변환법` 을 알아보자

### 해시 후 충돌 해소

긴 URL을 줄이려면 원래 URL을 줄이는 해시 함수가 필요하다.

손쉬운 방법은 세간에 잘 알려진 CRC32, MD5, SHA-1같은 해시 함수를 이용하는 것인데,

**이 함수들로 생성된 문자열은 7글자를 초과하기에 위에서 정한 n=7을 만족할 수 없다**

그래서 단순하게는 해시 값에서 처음 7개만 사용하는 방법이 있겠지만
→ 이는 충돌 확률이 높아진다.

이럴 때 충돌이 해소될 때까지 사전에 정한 `문자열을 덧붙여 충돌을 해소`할 수 있다

하지만 이 방법은 충돌 여부 파악을 위해 한 번 이상 DB에 쿼리를 해야하므로 오버헤드가 크다.

따라서 DB대신에 `블룸 필터`를 이용해볼 수 있다.

- `블룸 필터`
  - 어떤 집합에 특정 원소가 있는지 검사할 수 있도록 하는 확률론에 기초한 기술
  - 공간 효율이 좋음

### base-62 변환

진법 변환(base conversiond)은 URL 단축기를 구현할 때 흔히 사용되는 접근법 중 하나이며
수의 표현 방식이 다른 두 시스템이 같은 수를 공유하여야 하는 경우 유용하다.

추가로 62진법을 쓰는 이유는 hashValue에서 표현가능한 문자 수가 62개이기 때문이다.

- 단 이 경우 `유일성 보장 ID 생성기`가 별도로 필요하다

### 해시 후 충돌 해소 vs base-62 변환

`해시 후 충돌 해소`와 `base-62` 는 각각의 장단점이 있어 트레이드오프가 필요하다

아래 장단점을 비교하여 상황에 맞는 적절한 선택을 하자

| 해시 후 충돌 해소 전략                                                    | base-62 변환                                                                      |
| ------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
| 단축 URL의 길이가 고정됨                                                  | 단축 URL의 길이가 ID 값에 따라 가변적                                             |
| 유일 ID 생성기 필요 X                                                     | 유일 ID 생성기 필요 O                                                             |
| 충돌 가능 = 충돌 해소 전략 필요                                           | ID 유일성이 보장된 후에 적용할 수 있으므로 이론상 충돌 불가능                     |
| ID로부터 URL을 계산하는 방식이 아니므로 다음에 쓸 수 있는 URL 유추 불가능 | ID가 1씩 증가하는 값이라고 가정하면 다음 URL을 쉽게 알아낼 수 있어 보안 문제 존재 |

### 마치며

위에서 알아본 내용을 바탕으로 URL 단축기를 설계해볼 수 있을 것 같다.

면접에서 설계를 마친 이후 시간이 남는다면 다음 논의들을 해보자

- `처리율 제한 장치`(rate limiter)
  - 엄청난 양의 URL 단축 요청이 밀려들 경우 URL 단축기 시스템이 무력화될 수 있다.
  - IP 주소를 비롯한 필터링 규칙으로 요청을 걸러내보자
- `웹 서버의 규모 확장`
  - 웹 계층은 무상태 계층이므로 자유로이 증설/삭제 가능하다
- `데이터베이스 규모 확장`
  - 데이터베이스를 다중화하거나 샤딩하여 규모 확장을 해볼 수 있다
- `데이터 분석 솔루션`
  - 비즈니스 성장을 위해 데이터는 중요하다
  - URL 단축기에 대해서 어떤 링크를 얼마나 많은 사용자가 클릭하는지 중요한 지표들을 알아볼 수 있다
- `가용성`, `데이터 일관성`, `안정성`
  - 안정적인 대규모 시스템을 위한 필수 요소인 가용성, 데이터 일관성, 안정성에 대해서도 이야기 해볼 수 있을 것 같다.

# 9장 웹 크롤러 설계

웹 크롤러(web crawler)를 설계해보자.

웹 크롤러는 로봇 또는 스파이더라고도 불리며 검색 엔진에 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적이다.

크롤러는 다양하게 이용된다.

- `검색 엔진 인덱싱(search engine indexing)`
  - 크롤러의 가장 보편적인 사용사례로 웹페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다
  - Googlebot은 구글이 사용하는 웹 크롤러다
- `웹 아카이빙(web archiving)`
  - 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차를 말한다.
  - 많은 국립 도서관이 크롤러를 돌려 웹사이트를 아카이빙하고 있다.
- `웹 마이닝(web mining)`
  - 웹의 폭발적 성장세는 데이터 마이닝업계에 전례 없는 기회다. 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해 낼 수 있는 것이다.
  - 유명 금융 기업들은 크롤러를 사용해 주주 총회 자료나 연차 보고서(annual report)를 다운 받아 기업의 핵심 사업 방향을 알아내기도 한다
- `웹 모니터링`
  - 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다

웹 클로러의 복잡도는 처리해야 하는 데이터의 규모에 따라 달라진다.

몇 시간이면 끝낼 수 있는 작은 학급 프로젝트 수준일 수도 있고, 별도의 엔지니어링 팀을 꾸려서 지속적으로 관리하고 개선해야 하는 초대형 프로젝트가 될 수도 있다.

따라서 우선 우리가 설계할 웹 크롤러가 감당해야 하는 데이터의 규모와 기능들을 파악해야 한다.

## 웹 크롤러 설계하기

웹 크롤러의 기본 알고리즘은 간단하다

1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 페이지를 모두 다운로드 한다
2. 다운받은 웹 페이지에서 URL들을 추출한다
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다

**웹 크롤러가 감당해야 하는 데이터의 규모와 기능 요구사항을 만족해야 하고 다음과 같은 속성에 주의를 기울이는 것도 바람직하다.**

- `규모 확장성`
  - 오늘날 웹에는 수십억 개의 페이지가 존재하는 것으로 알려져 있다. 따라서 병행성(parallelism을 활용하면 보다 효과적으로 웹 크롤링을 할 수 있을 것이다
- `안정성(robustness)`
  - 웹은 함정으로 가득하다. 잘못 작성된 HTML, 아무 반응이 없는 서버, 장애, 악성 코드가 붙어있는 링크 등이 그 좋은 예다.
  - 크롤러는 이런 비정상적인 입력이나 환경에 잘 대응할 수 있어야 한다
- `예절(politeness)`
  - 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안 된다
- `확장성(extensibility)`
  - 새로운 형태의 컨텐츠를 지원하기가 쉬워야 한다. 예를 들어, 이미지 파일도 크롤링하고 싶다고 해 보자. 이를 위해 전체 시스템을 새로 설계해야 한다면 곤란할 것이다.

## 크롤러 설계안

요구사항이 분명해지면 개략적 설계를 진행해보자

아래 구조는 웹 크롤러에 관한 선행연구를 참고한 것이다.

아래 다이어그램에 등장하는 컴포넌트 각각이 어떤 기능을 수행하는지 살펴보자

![Untitled](https://github.com/BOAZ-bigdata/24-1_Study_Large-ScaleSystemDesign/assets/72328687/31f01c34-f797-4b3e-a5c4-fcfc85084f62)

### 시작 URL 집합

시작 URL 집합은 웹 크롤러가 크롤링을 시작하는 출발점이다.

**예를 들어 어떤 대학 웹사이트로부터 찾아 나갈 수 있는 모든 웹 페이지를 크롤링하는 가장 직관적인 방법은?**

→ 해당 대학의 도메인 이름이 붙은 모든 페이지의 URL을 시작 URL로 쓰는 것이다.

**전체 웹을 크롤링해야 하는 경우에는 시작 URL을 고를 때 좀 더 창의적일 필요가 있다.**

크롤러가 가능한 한 많은 링크를 탐색할 수 있는 URL을 고르는 것이 바람직할 것이다.

**일반적으로 전체 URL 공간을 작은 부분집합으로 나누는 전략을 쓴다.**

- 지역적인 특색, 즉 나라별로 인기 있는 웹 사이트가 다르다는 점을 활용
- 주제별로 다른 시작 URL을 사용하기. e.g. URL 공간을 쇼핑, 스포츠, 건강 등의 주제별로 세분화하고 그 각각에 다른 시작 URL을 적용

시작 URL로 무엇을 쓸 것이냐는 질문에 정답은 없다. 면접관도 완벽한 답안을 기대하는 것은 아니니 의도가 무엇인지만 정확히 전달하자

### 미수집 URL 저장소

대부분 현대적 웹 크롤러는 크롤링 `상태를 다운로드할 URL`, 그리고 `다운로드된 URL`의 두 가지로 나눠 관리한다. 이 중 다운로드할 URL을 저장 관리하는 컴포넌트를 미수집 URL 저장소(URL frontier)라고 부른다. FIFO 큐라고 생각하면 된다.

### HTML 다운로더

HTML 다운로더(downloader) 인터넷에서 웹 페이지를 다운로드하는 컴포넌트다

다운로드할 페이지의 URL은 미수집 URL 저장소가 제공한다.

### 도메인 이름 변환기

웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요하다. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL에 대응되는 IP 주소를 알아낸다.

### 컨텐츠 파서

웹 페이지를 다운로드 하면 파싱(parsing)과 검증(validation) 절차를 거쳐야 한다.

이상한 웹 페이지는 문제를 일으킬 수 있고 저장 공간만 낭비하게 되기 ㅈ때문이다

크롤링 서버 안에 컨텐츠 파서를 구현하면 크롤링 과정이 느려지게 될 수 있으므로 독립된 컴포넌트로 구성한다

### 중복 컨텐츠인가?

웹에 공개된 연구 결과에 따르면, 29% 가량의 웹 페이지 컨텐츠는 중복이다.

데이터 중복은 저장소 낭비이므로 피해야 한다.

HTML 문서를 비교하는 가장 간단한 방법은 그 두 문서를 문자열로 보고 비교하는 것이겠지만, 비교 대상 문서의 수가 10억에 달하는 경우에는 느리고 비효율적어서 적용하기 어렵다.

**여기서 효과적인 방법은 웹페이지의 해시 값을 비교하는 것이다.**

### 컨텐츠 저장소

HTML 문서를 보관하는 시스템이다.

**저장소를 구현하는 데 쓰일 기술을 고를 때는 저장할 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려해야 한다.**

현재 설계안에서는 데이터 양이 너무 많으므로 대부분의 컨텐츠는 디스크에 저장하고 인기 있는 컨텐츠는 메모리에 두어 접근 지연시간을 줄인다.

### URL 추출기

HMTL 페이지를 파싱하여 링크들을 골라내는 역할을 한다.

상대 경로는 모두 prefix를 붙여 절대 경로로 변환하는 등의 역할도 해야한다.

### URL 필터

특정한 컨텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제어 목록(deny list)에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 한다.

### 이미 방문한 URL?

이미 방문한 URL 여부를 체크해서 중복 처리를 방지하기 위해 이미 방문한 URL 이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있는 자료 구조를 사용한다.

**블룸 필터(bloom filter)나 해시 테이블이 널리 쓰인다.**

### URL 저장소

URL 저장소는 이미 방문한 URL을 보관하는 저장소다

### 웹 탐색 시 DFS vs BFS

웹은 유향 그래프(directed graph)나 같다. 페이지는 노드이고 하이퍼링크는 엣지라고 보면 된다. 크롤링 프로세스는 이 유향 그래프를 엣지를 따라 탐색하는 과정이다.

DFS(depth-first search), BFS(breath-first search)는 그래프 탐색에 널리 사용되는 두 알고리즘이다.

하지만 DFS는 그래프 크기가 클 경우 어느 정도로 깊숙이 가게 될지 가늠이 어려워서 좋은 선택이 아닐 가능성이 높다. **따라서 웹 크롤러는 보통 BFS를 사용한다.**

**BFS로 구현하는 것에 문제점도 있으니 주의하자.**

- 한 페이지에서 나오는 링크의 상당수는 같은 서버(하위 페이지 등)로 돌아가는데, 이 때 링크들을 병렬로 처리하게 된다면 같은 서버에 수많은 요청으로 과부하에 걸릴 수 있다.
  - **이런 크롤러는 보통 예의 없는(impolite) 크롤러로 간주된다.**
- 표준적 BFS 알고리즘은 URL 간에 우선순위를 두지 않는다. 하지만 모든 웹 페이지가 같은 수준의 품질, 같은 수준의 중요성을 갖지는 않는다.
  - **그래서 페이지 순위나 유저 트래픽 양, 업데이트 빈도 등을 여러 가지 척도에 두어 처리 우선순위를 구별하는 것이 적합할 수 있다.**

### 미수집 URL 저장소

미수집 URL 저장소. 즉 앞으로 다운로드할 URL을 저장해서 활용하면 위에서 본 문제를 좀 해결할 수 있다.

**이 저장소를 잘 구현하면 예의(politeness)를 갖춘 크롤러, URL 사이의 우선순위와 신선도(freshness)를 구별하는 크롤러를 구현할 수 있다.**

**미수집 URL 저장소의 구현 방법에 대해서는 논문도 다수 나와 있는데, 이 가운데 중요한 것을 요약하면 다음과 같다.**

`예의`

웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것을 삼가야 한다. 이는 무례한 일이며 떄로는 DoS(Denial-of-Service) 공격으로 간주되기도 한다.

`예의 바른 크롤러를 만드는데 지켜야 할 한 가지 원칙`은 **동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청한다는 것이다. 같은 웹 사이트의 페이지를 다운받는 태스트는 시간차를 두고 실행하면 될 것이다.**

이 요구사항을 만족하려면 웹 사이트의 호스트명과 다운로드를 수행하는 작업 스레드 간의 관계를 유지하면 된다.

**위 요구사항을 만족하기 위해 FIFO 큐와 호스트-큐 매핑 테이블을 두고 같은 호스트에 해당하는 URL은 같은 큐에 모이도록 설계한다.**

`우선순위`

중요한 페이지를 먼저 수집하기 위해 페이지 랭크, 트래픽 양, 개선 빈도(update frequency) 등 다양한 척도를 사용해서 `순위결정장치(prioritizer)`를 만든다.

**우선순위 별로 큐가 하나씩 할당되고 순위결정장치에서 우선순위 별로 큐에 URL을 추가한다.** `큐 선택기`**에서 임의 큐에서 처리할 URL을 꺼내도록 하고 순위가 높은 큐에서 더 자주 꺼내도록 가중치를 둔다.**

`신선도`

웹 페이지는 수시로 추가되고, 삭제되고, 변경된다. 따라서 데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지도 주기적으로 재수집할 필요가 있다.

이를 위한 전략은 다음과 같다

- 웹 페이지의 변경 이력 활용
- 우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집

`미수집 URL 저장소를 위한 지속성 저장장치`

검색 엔진을 위한 크롤러의 경우, 처리해야 하는 URL의 수는 수억 개에 달한다. 이 모두를 메모리에 보관하는 것은 안정성이나 규모 확장성 측면에서 바람직하지 않다.

**그렇다고 전부 디스크에 저장하는 건 느려서 쉽게 성능 병목지점이 되기 때문에 좋은 방법은 아니다.**

이를 위한 **절충안(hybrid approach)**으로 대부분의 URL은 디스크에 두지만 IO 비용을 줄이기 위해 메모리 버퍼에 큐를 두는 것도 방법이다. 그리고 버퍼에 있는 데이터는 주기적으로 디스크에 기록될 것이다.

### HTML 다운로더

HTMl 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려 받는다.

이제 HTML 다운로더에서 중요한 요소들을 알아봐야 한다. 먼저 `로봇 제외 프로토콜`부터 살펴보자

`Robots.txt`

로봇 제외 프로토콜이라고 부르기도 하는 Robots.txt는 웹사이트가 크롤러와 소통하는 표전적 방법이다. **이 파일에는 크롤러가 수집해도 되는 페이지 목록이 들어있다.**

따라서 웹 사이트를 긁어 가기 전에 크롤러는 해당 파일에 나열된 규칙을 먼저 확인해야 한다.

`성능 최적화`

HTML 다운로더에 사용할 수 있는 성능 최적화 기법

1. 분산 크롤링
    1. 성능을 높이기 위해 크롤링 작업을 여러 서버에 분산한다.
2. 도메인 이름 변환 결과 캐시
    1. **도멩인 이름 변환기(DNS Resolver)는 크롤러 성능의 병목 중 하나인데, 이는 DNS 요청을 보내고 결과를 받는 작업의 동기적 특성 때문이다.**
    2. DNS 요청이 처리되는 데는 보통 10ms~200ms가 소요되고 크롤러 스레드 가운데 어느 하나라도 이 작업을 하고 있으면 다른 스레드의 DNS 요청은 block된다.
    3. 따라서 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해 놓고 크론 잡(cronjob) 등을 돌려 주기적으로 갱신하도록 하면 성능을 높일 수 있다.
3. `지역성`
    1. 크롤링 작업을 수행하는 서버를 지역별로 분산한다. 크롤링 서버가 대상 서버와 지역적으로 가까우면 페이지 다운로드 시간은 줄어들 것이다.
4. `짧은 타임아웃 두기`
    1. 어떤 웹 서버는 응답이 느리거나 아예 응답하지 않는다. 이런 경우 대기 시간이 길면 좋지 않으므로 최대 얼마나 기다릴지를 미리 정해둔다.

`안정성`

최적화된 성능 뿐만 아니라 안정성도 다운로더 설계 시에 중요하다. 시스템 안정성을 향상시키기 위한 접근법 가운데 중요한 몇 가지는 다음과 같다.

1. `안정 해시`
    1. 다운로더 서버들에 부하를 분산할 때 안정 해시를 통해 다운로더 서버를 쉽게 추가하고 삭제한다.
2. `크롤링 상태 및 수집 데이터 저장`
    1. 장애가 발생한 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해 두는 것이 바람직하다.
    2. 저장된 데이터를 로딩하고 나면 중단되었던 크롤링을 쉽게 재시작할 수 있을 것 이다.
3. `예외 처리`
    1. 대규모 시스템에서 에러는 불가피할 뿐만 아니라 흔하게 벌어지는 일이다. 따라서 예외가 발생해도 전체 시스템이 중단되는 일 없이 그 작업을 우아하게 이어나갈 수 있어야 한다.
4. `데이터 검증`
    1. 시스템 오류를 방지하기 위한 중요 수단 가운데 하나이다.

`확장성`

HTML 다운로더는 새로운 형태의 컨텐츠를 지원해야할 수도 있다. 따라서 새로운 형태의 컨텐츠를 쉽게 지원할 수 있도록 새로운 모듈을 끼워 넣어서 설계하자

다음은 모듈 예시다

- **PNG 다운로더는 PNG 파일을 다운로드 하는 플러그인 모듈**
- **웹 모니터는 웹을 모니터링하여 저작권이나 상표권이 침해되는 일을 막는 모듈**

![Untitled](https://github.com/BOAZ-bigdata/24-1_Study_Large-ScaleSystemDesign/assets/72328687/bbcee4ab-4cb9-4f94-af81-cce837b47099)

`문제 있는 컨텐츠 감지 및 회피`

중복이거나 의미 없는, 또는 유해한 컨텐츠를 어떻게 감지하고 시스템으로부터 차단할지 살펴보자

1. `중복 컨텐츠`
    1. 해시나 체크섬(check-sum)을 사용해 중복 컨텐츠를 보다 쉽게 탐지할 수 있다
2. `거미 덫(spider trap)`
    1. spider trap은 크롤러를 무한 루프에 빠지도록 설계한 웹 페이지다. 이런 덫은 URL의 최대 길이를 제한하면 회피할 수 있다. 하지만 가능한 모든 종류의 덫을 피할 수 있는 만능 해결책은 없다.
    2. 이런 페이지는 기이할 정도로 웹 페이지를 가지고 있는 것이 일반적이라 알아내는 것은 어렵지 않다. 하지만 덫을 자동으로 피해가는 알고리즘을 만들어 내는 것은 까다롭다.
    3. 한 가지 방법은 사람이 수작업으로 덫을 확인하고 찾아낸 후 해당 사이트를 크롤러 탐색 대상에서 제외하거나 URL 필터 목록에 걸어두는 것이다.
3. `데이터 노이즈`
    1. 어떤 컨텐츠는 거의 가치가 없다. 광고나 스크립트 코드, 스팸 URL 같은 것이 그렇다. 이런 것들으 도움될 것이 없으므로 가능하다면 제외해야 한다.

## 더 고려해볼만한 것들

- `서버 사이드 렌더링`
  - 많은 웹 사이트가 동적으로 페이지를 만든다. 이렇게 만들어진 페이지를 그대로 다운 받으면 동적으로 생성되는 링크는 발견할 수 없다.
  - 이런 문제는 페이지를 파싱하기 전에 서버 사이드 렌더링을 적용하면 해결할 수 있다.
- `원치 않은 페이지 필터링`
  - 저장 공간 등 크롤링에 소요되는 자원은 유한하기 때문에 스팸 방지 컴포넌트를 두어 품질이 나쁘거나 스팸성인 페이지를 걸러내도록 해두면 좋다.
- `데이터베이스 다중화 및 샤딩`
  - 다중화, 샤딩 같은 기법을 적용하면 데이터 계층의 가용성, 규모 확장성, 안정성이 향상된다.
- `수평적 규모 확장성`
  - 대규모의 크롤링을 위해서 다운로드를 실행할 서버가 수백 혹은 수천 대 필요하게될 수도 있다.
  - 수평적 규모 확장성을 달성하는 데 중요한 것은 서버가 상태정보를 유지하지 않도록 stateless 서버로 만드는 것이다.
- `가용성, 일관성, 안정성`
  - 가용성, 일관성, 안정성은 성공적인 대형 시스템을 만들기 위해 필수적으로 고려해야 하는 것들이다.
- `데이터 분석 솔루션`
  - 데이터를 수집하고 분석하는 것은 어느 시스템에게나 중요하다. 시스템을 세밀히 조정하기 위해서는 이런 데이터와 그 분석 결과가 필수적이다.
