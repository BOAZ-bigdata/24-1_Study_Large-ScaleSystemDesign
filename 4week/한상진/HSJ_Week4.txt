# 도입

## 키-값 저장소

- 저장소에 저장되는 값은 고유 식별자를 키로 가져야 함
- 해당 키와 연결된 값은 키를 통해서만 접근 가능
- 키는 텍스트 or 해시 값일 수 있다 (성능상 짧을수록 좋다.)
    - 일반 텍스트키 - "last_logged_in_at"
    - 해시 키 - 253DDEC4
- 값으로는 다양한 값이 될수 있다 (문자열, 리스트, 객체...)
- 아마존 다이나모, memcached, 레디스 등이 대표적인 예

# 단일 서버 키-값 저장소

- 키-값 쌍 전부를 메모리에 해시 테이블로 저장하는 방식.
    - 장점
        - 빠른 접근 속도 보장
    - 단점
        - 모든 데이터를 메모리에 둘 수 없음.

# 분산 키-값 저장소

## CAP 정리

- C,A,P 이들 중 어떤 2가지를 충족하려면 나머지 하나는 반드시 희생되어야 한다는 것을 의미.

### Consistency(데이터 일관성)

- 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접근하든 동일한 데이터.
- 예시
    - 통신사가 있고 우리가 그 고객이라고 생각해보자. 어떤 상담원과 연결됐더라도, **같은 정보를 공유.**

### Availability(가용성)

- 분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야 한다.
- 예시
    - 통신사가 있고 우리가 그 고객이라고 생각해보자. 통신사 고객센터는 언제든지 연락 가능하다. 고객은 필요한 정보를 고객센터를 통해 **언제든지** 알아낼 수 있는 것.

### Partition Tolerance (파티션 감내성)

- 네트워크에 파티션(장애)이 생기더라도 시스템은 계속 동작.
    - 파티션 : 두 노드 사이에 통신 장애가 발생하였음을 의미.
- 분산 시스템은 Partition Tolerance를 무조건 지키도록 설계되어야 함.

## 시스템 컴포넌트

## 데이터 파티션

- 대규모 어플리케이션의 경우 데이터를 작은 파티션으로 분할하여 여러 서버에 저장.
- 데이터를 파티션 단위로 나눌 때 안정 해시를 이용
    - 규모 확장 자동화, 다양성의 장점

## 데이터 다중화

- 높은 가용성과 안정성을 확보하기 위해서는 데이터 다중화가 필요
    - 여러 서버에 동일한 데이터가 저장되게…
- 안정해시 방법으로 시계방향으로 순회하면서 n개 서버에 데이터 사본을 저장.
    - 다만 가상 노드들이 존재함으로 같은 물리서버를 선택하지 않도록 해야 한다.

## 데이터 일관성

- 여러 노드에 다중화된 데이터는 적절히 동기화가 필요.
- 다중화된 노드에 데이터를 분산 저장할 때, 정족수 합의 (Quorum Consensus) 알고리즘을 이용해서 일관성 수준을 조절

### **정족수 합의**

- N = 사본 개수
- W = 쓰기 연산에 대한 정족수. 쓰기 연산이 성공한 것으로 간주되려면 적어도 W개의 서버로부터 쓰기 연산이 성공했다는 응답을 받아야 한다.
    - W = 1인 경우쓰기 연산이 성공했다고 판단하기 위해서는 1대의 서버로 부터 쓰기 연산 성공의 응답을 받아야함.
- R = 읽기 연산에 대한 정족수. 읽기 연산이 성공한 것으로 간주되려면 적어도 R개의 서버로부터 응답 받아야 한다.
- • W + R > N인 경우 강한 일관성이 보장됨

### **일관성 모델**

**강한 일관성**

- 모든 읽기 연산은 가장 최근의 결과를 반환
- 모든 사본에 현재 쓰기 연산의 결과를 반영하기 전까지는 읽기/쓰기 금지
    
    → 고가용성 시스템에 적합하지 않다.
    

**약한 일관성**

- 읽기 연산의 결과가 가장 최근에 갱신된 결과를 반영하지 못할 수 있음.

**결과적 일관성**

- 약한 일관성의 한 형태로, 갱신 결과가 결국에는 모든 사본에 반영(동기화) 되는 모델
- 쓰기 연산이 병렬적으로 발생하면 일관성이 깨질 수 있는 문제가 발생.

## 일관성 불일치 해소

### **비 일관성 해소 기법 : 데이터 버저닝**

- 데이터를 다중화하면, 가용성은 높아지지만 사본 간 일관성이 깨질 가능성이 높아짐.
- 일관성을 유지하기 위해 버저닝과 벡터 시계 도입.

- [서버, 버전]의 순서쌍을 통해 클라이언트가 충돌이 해소 후 서버에 기록

## 장애처리

- 대다수 대규모 시스템에서 장애는 흔하게 발생.
- 장애 처리를 위해서는 장애 감지와 장애 해소 전략이 필요.

### 장애 감지

1.  **모든 노드 사이에 멀티캐스팅 채널을 구축**
    - 서버가 많을 때는 비효율적입니다.
    - 가십 프로토콜같은 분산형 장애감지 솔루션을 채택하는 것이 더 효율적.
2. **가십 프로토콜**
    - 분산형 장애 감지 솔루션.
    - 동작 원리
        - 각 노드는 멤버십 목록을 유지. 멤버십 목록은 각 멤버 ID와 그 박동 카운터(heartbeat counter) 쌍의 목록
        - 각 노드는 주기적으로 자신의 박동 카운터를 증가시킴
        - 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보냄
        - 박동 카운터 목록을 받은 노드는 멤버십 목록을 최신 값으로 갱신
        - 어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 해당 멤버는 장애 상태(Offline)인 것으로 간주

### 장애 해소

- 일시적 장애 처리 ← 임시 위탁 기법 이용
    - 다른 서버(노드)가 잠시동안 대신 맡아서 처리.
    - 그동안 발생한 변경사항은 해당 서버가 복구되었을 때 일괄 반영하여 일관성을 보존.
- 영구적 장애 처리
    - 반-엔드로피(anti-entropy) 프로토콜을 구현하여 사본들을 동기화 합니다.
    - 사본들을 비교해 최신 버전으로 갱신하는 과정을 포함 합니다.
    - 사본 간 일관성이 망가진 상태를 탐지하고 전송 데이터의 양을 줄이기 위해 머클 트리 사용.

# 분산 시스템을 위한 유일 ID 생성

- 분산환경에서는 RDB의 atuo_increment와 같은 접근법이 통하지 않음.
    - 데이터베이스를 1대만 사용하지 않는다.
    - 여러 데이터베이스 서버를 쓰는 경우에는 지연시간을 낮추기가 무척 힘들다.

## 1단계 : 문제 이해 및 설계 범위 확정

- 면접관의 질문을 통해 ID에 대한 정보를 획득

## 2단계 : 개략적 설계안 제시 및 동의 구하기

### 다중 마스터 복제

- 다중 마스터 복제 방법은, auto_increment 를 사용하지만, 다음 ID 값을 +1 증가시켜 얻는게 아닌 +n 값을 증가시킴
    - 여기서 n 은 현재 사용중인 데이터베이스 서버의 개수.
- 단점
    - 여러 데이터 센터에 걸쳐 규모를 늘리기 어렵다.
    - ID 의 유일성은 보장되지만 그 값이 시간 흐름에 맞추어 커지도록 보장할 수 없다.
    - 서버를 추가하거나 삭제할 때도 잘 동작하도록 만들기 어렵다.

### UUID

- 정보를 유일하게 식별하기 위한 128비트짜리 수
- 중복 UUID가 발생하기 매우 희박하다.
- 단점
    - ID 가 128비트로 길다
    - ID 를 시간순으로 정렬할 수 없다.
    - ID 에 숫자가 아닌 값이 포함될 수 있다.

### 티켓 서버

- auto_increment 기능을 갖춘 데이터베이스 서버에서 id를 분산시킴.
- 단점
    - SPOF 문제

### 트위터 스노플레이크 접근법

- ID의 구조를 여러 Section으로 분할하여 채움.
- 생성순으로 정렬이 가능

## 3단계 : 상세 설계

- 트위터 스노플레이크 접근법을 이용해 상세 설계 진행

### 타임 스탬프

- 41비트로 표현.
- 타임 스탬프는 시간이 흐름에 따라 더 큰 값을 갖으므로 ID는 시간순으로 정렬이 가능
    - 최신 값일수록 ID값이 큼
- 41 bit로 표현할 수 있는 타임스탬프의 최대가 된다면 ID 체계 or 기원시각을 바꿔야 함.

## 4단계 : 마무리

- 시계 동기화, 각 절의 길이 최적화, 고가용성 등을 더 이야기….
