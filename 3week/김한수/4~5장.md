# 4~5장

- [4~5장](#45장)
- [4장 처리율 제한 장치의 설계](#4장-처리율-제한-장치의-설계)
  - [처리율 제한 장치는 어디에 둘 것인가?](#처리율-제한-장치는-어디에-둘-것인가)
  - [처리율 제한 알고리즘](#처리율-제한-알고리즘)
    - [토큰 버킷 알고리즘](#토큰-버킷-알고리즘)
    - [누출 버킷 알고리즘](#누출-버킷-알고리즘)
    - [고정 윈도 카운터 알고리즘](#고정-윈도-카운터-알고리즘)
    - [이동 윈도 로깅 알고리즘](#이동-윈도-로깅-알고리즘)
    - [이동 윈도 카운터 알고리즘](#이동-윈도-카운터-알고리즘)
  - [상세 설계](#상세-설계)
  - [분산 환경에서의 처리율 제한 장치 구현](#분산-환경에서의-처리율-제한-장치-구현)
  - [성능 최적화](#성능-최적화)
  - [모니터링](#모니터링)
  - [요약 \& 추가 TIP](#요약--추가-tip)
- [5장 안정 해시 설계](#5장-안정-해시-설계)
  - [해시 키 재배치(refresh) 문제](#해시-키-재배치refresh-문제)
  - [안정 해시](#안정-해시)
    - [해시 공간과 해시 링](#해시-공간과-해시-링)
    - [해시 서버 조회](#해시-서버-조회)
    - [서버 추가와 삭제](#서버-추가와-삭제)
    - [가상 노드를 활용한 균등 분포](#가상-노드를-활용한-균등-분포)
  - [요약](#요약)
  - [안정 해시 사용 사례](#안정-해시-사용-사례)

# 4장 처리율 제한 장치의 설계

- `처리율 제한 장치(rate limiter)` :  네트워크 시스템에서 클라이언트 또는 서비스가 보내는 트래픽의 처리율(rate)을 제어하기 위한 장치
- **요구사항 예시**
  - 초당 2회 이상 새 글을 올릴 수 없다.
  - 같은 IP 주소로는 하루에 10개 이상의 계정을 생성할 수 없다.
- 이점
  - Dos(Denial of Service) 공격에 대한 자원 고갈(resource starvation)을 방지
  - 비용 절감
    - 제한된 처리율에 맞게 서버 수를 조정 가능
    - 비용 지불이 필요한 서드파티 API의 경우 비용 제어 가능
  - 서버 과부하 제어
    - 봇(bot)으로 오는 트래픽이나 사용자의 잘못된 이용 패턴으로 유발된 트래픽을 걸러낼 수 있음

## 처리율 제한 장치는 어디에 둘 것인가?

- 클라이언트, 서버 어디서든 구현할 수 있지만 클라이언트 측에 두는 건 악의적인 케이스가 발생할 수 있어 신뢰도가 떨어짐
- 따라서 서버 측에서 구현하는 것이 이상적이고 마이크로서비스에서는 게이트웨이 형태로 자주 구현함
  - **하지만 늘 그렇듯 구현 방법에 대한 정답은 없다. 완전 관리형 서비스를 사용할 수도 있고 각 조직의 현재 상황에 맞게 잘 선택하자.**

## 처리율 제한 알고리즘

- 처리율 제한을 구현하는 알고리즘은 여러 가지이고 각각 장단점이 있다.
- 다음 알고리즘에 대해서 알아보자.
  - 토큰 버킷(token bucket)
  - 누출 버킷(leaky bucket)
  - 고정 윈도 카운터(fixed window counter)
  - 이동 윈도 로그(sliding window log)
  - 이동 윈도 카운터(sliding window counter)

### 토큰 버킷 알고리즘

토큰 버킷 알고리즘은 버킷에 토큰을 모아두고 각 요청 당 1개의 토큰을 소모하는 개념으로 처리율을 제한함

- 지정된 용량을 가진 토큰 버킷이 존재함
- 사전 설정된 양의 토큰이 버킷에 주기적으로 채워짐
- 버킷의 용량을 초과하면 토큰이 추가되지 않음
- 각 요청이 처리될 때 마다 하나의 토큰을 사용함
- 토큰이 없는 경우 해당 요청은 버려짐
- `장점`
  - 구현이 쉽고 메모리 사용 측면에서 효율적
  - 짧은 시간에 집중되는 트래픽(burst of traffic)도 처리 가능
    - 버킷에 남은 토큰이 있기만 하면 요청이 시스템에게 전달됨
- `단점`
  - 버킷 크기와 토큰 공급률이라는 두 개의 파라미터를 가지는데, 이 값을 적절하게 튜닝하는 것이 까다로움

### 누출 버킷 알고리즘

보통 FIFO 큐 구조로 구현되며 일정한 고정 속도로 요청을 처리하여 처리율을 제한함

토큰 버킷 알고리즘과 비슷하지만 요청 처리율이 고정되어 있다는 점에서 다름

- 요청이 동작하면 큐가 가득 차 있는지 확인하고 빈자리가 있는 경우에는 큐에 요청을 추가함
- 큐가 가득 차 있는 경우 새 요청을 버림
- 지정된 시간마다 큐에서 요청을 꺼내어 처리함
- `장점`
  - 큐의 크기가 제한되어 있어 메모리 사용량 측면에서 효율적
  - 고정된 처리율이므로 안정적 출력이 필요한 경우에 적합
- `단점`
  - 단시간에 많은 트래픽이 몰리는 경우 큐에 오래된 요청들이 쌓이고 이를 제때 처리 못하면 최신 요청들은 버려지게 됨
  - 버킷 크기, 처리율을 튜닝하기 까다로울 수 있음

### 고정 윈도 카운터 알고리즘

타임라인을 고정된 간격의 윈도우(window)로 나누고 각 윈도 안의 요청을 카운트하는 방식으로 처리율을 제한함

- 요청이 접수될 때 마다 각 윈도우의 카운터 값이 1씩 증가하고
- 카운터 값이 임계치(threshold)에 도달하면 새로운 요청은 새 윈도가 열릴 때까지 버려짐
- 윈도우가 고정되어 있으므로 윈도우의 경계 부근에 순간적으로 많은 트래픽이 집중될 경우 윈도우에 할당된 양보다 더 많은 요청이 처리될 수 있음.
- `장점`
  - 메모리 효율이 좋음
  - 이해하기 쉬움
  - 윈도가 닫히는 시점에 카운터를 초기화하는 방식이 어울리는 트래픽 패턴을 처리하기에 적합함
- `단점`
  - 윈도 경계 부근에서 일시적으로 많은 트래픽이 몰리면 예상보다 더 많은 양의 요청을 처리할 수도 있음

### 이동 윈도 로깅 알고리즘

고정 윈도 카운터 알고리즘은 트래픽이 집중되는 경우에 대한 문제점이 있는데 이동 윈도 로깅 알고리즘은 이 문제를 해결할 수 있음

- 요청의 타임스탬프를 추적함. 보통 레디스의 정렬 집합(sorted set) 같은 캐시에 보관함.
- 현재 윈도의 시작 지점보다 오래된 타임스탬프는 만료된 걸로 보고 새 요청이 오면 만료된 타임스탬프를 제거함.
- 새 요청의 타임스탬프를 로그(log)에 무조건 추가
- 로그의 크기가 허용치보다 작으면 요청을 시스템에 전달, 반대의 경우는 거부
- `장점`
  - 처리율 제한 메커니즘이 아주 정교하여 지정한 처리율 한도를 넘지 않음
- `단점`
  - 거부된 타임스탬프도 보관하기 때문에 다량의 메모리를 사용함

### 이동 윈도 카운터 알고리즘

고정 윈도 카운터 알고리즘과 이동 윈도 로깅 알고리즘을 결합한 알고리즘.

- 여러 접근법이 사용될 수 있는데, 기본적으로 윈도우를 이동하며 처리량을 카운트하는 게 포인트
- `장점`
  - 이전 시간대의 평균 처리율에 따라 현재 윈도의 상태를 계산하므로 짧은 시간에 몰리는 트래픽에도 잘 대응함
  - 메모리 효율이 좋음
- `단점`
  - 직전 시간대에 도착한 요청이 균등하게 분포되어 있다고 가정한 상태에서 계산하기 때문에 다소 느슨함(휴리스틱) 하지만 이 문제는 cloudflare에서 실험한 내용에 따르면 실제 상태와 맞지 않게 요청이 허용되거나 버려진 경우는 0.003%에 불과함 (대부분 균등하게 분포되어 있다는 뜻)

## 상세 설계

- `처리율 한도 초과 트래픽의 처리`
  - HTTP 429(too many requests)응답
  - 한도 제한에 걸렸으나 꼭 처리해야하는 경우 따로 보관해뒀다가 나중에 처리할 수도 있음
- `처리율 제한 장치가 사용하는 HTTP 헤더`
  - 클라이언트에 처리율 제한 상태를 피드백해주는 것은 중요함. 이건 상황을 잘 판단해서 정의하자
  - **headers e.g.**
    - `X-Ratelimit-Remaining`: 윈도 내에 남은 처리 가능 요청 수
    - `X-Ratelimit-Limit`: 매 윈도마다 클라이언트가 전송할 수 있는 요청의 수
    - `X-Ratelimit-Retry-After`:  몇 초 뒤에 요청을 다시 보내야 제한에 걸리지 않는지 알림
  - 위 헤더를 보았을 때 429 응답과 X-Ratelimit-Retry-After를 헤더로 추가하는 것이 좋아보임

## 분산 환경에서의 처리율 제한 장치 구현

분산 환경에서는 항상 다음 두 가지 어려운 문제를 마주하게 됨

- `경쟁 조건(race condition)`
  - 해결하기 위해 lock을 사용하거나 (성능이 떨어짐)
  - 루아 스크립트(lua script) 또는 redis의 sorted set을 사용
- `동기화(synchronization)`
  - 고정 세션(stick session)을 활용해 한 서버만 바라보도록 할 수 있지만 이는 비효율적이고
  - 레디스와 같은 중앙 집중형 데이터 저장소를 써서 해결할 수 있음

## 성능 최적화

- 글로벌 서비스라면 물리적인 데이터센터 거리를 좁히기 위해 리전을 분리하여 엣지 서버를 활용할 수도 있음
- 이 경우 데이터 동기화 시 시간이 좀 걸리는데 최종 일관성(eventual consistency model)을 보장하도록 구성하는 것이 일반적임

## 모니터링

처리율 제한 장치가 원하는대로 잘 동작하고 있는지 파악하는 것도 중요함.

- 현재 요구사항에 사용하고 있는 처리율 알고리즘이 비효율적으로 동작한다면 이를 감지하고 알고리즘을 교체할 수 있음
- 옵저버빌리티는 항상 중요함

## 요약 & 추가 TIP

- 여러 처리율 제한 알고리즘을 알아봤음
  - 토큰 버킷
  - 누출 버킷
  - 고정 윈도 카운터
  - 이동 윈도 로그
  - 이동 윈도 카운터
- 더 알아보면 좋을 것들
  - 잠시 동안 임계치를 넘어서게 구성하여 처리율 제한을 느슨하게 하거나 딱딱하게 하는 케이스
  - 다양한 계층에서의 처리율 제한(OSI 7계층)
  - 클라이언트를 잘 구성해서 처리율 제한을 회피하는 방법
    - 클라이언트 캐시 사용으로 API 호출 횟수 감소
    - 처리율 제한을 이해하고 짧은 시간 동안 너무 많은 메시지를 보내지 않도록 하기
    - 예외나 에러를 처리하는 코드로 클라이언트가 예외적 상황에 잘 복구될 수 있도록 하기
    - 재시도(retry) 로직을 구현할 때 충분한 백오프(back-off) 시간 두기

# 5장 안정 해시 설계

수평적 규모 확장을 위해서 데이터를 균등하게 나누는 것도 중요한데 이 때 데이터를 나누는 기준 키값을 정하는 것이 중요하다. 그리고 일반적으로 이럴 때 해시 기술을 사용하는데, 이에 대해서 알아보자

## 해시 키 재배치(refresh) 문제

- 데이터가 너무 많아져서 해시 키가 변경해야 하는 상황이 되면, 변경할 해시 키에 맞춰 데이터를 재배치해야 하는 문제가 생긴다.
  - 이는 대량의 데이터의 쓰기 작업으로 인해 항상 어려운 문제이다.
- e.g. 4로 나눈 나머지 값을 기준으로 데이터를 나눠서 저장하고 있었다면 이를 3으로 나누도록 바꿨을 때 기준 값에 맞춰서 데이터 재배치가 일어나야한다.

## 안정 해시

안정 해시는 해시 테이블 크기가 조정될 때 평균적으로 오직 k/n개의 키만 재배치하는 해시 기술을 말한다.

- k는 키의 개수, n은 슬롯(slot)의 개수이다. 여기서 슬롯은 해시에 지정된 서버를 말한다.

### 해시 공간과 해시 링

- 해시 값에 따라 분배한 해시 공간을 1차원 배열로 나타낼 수 있는데 이 1차원 배열의 양 끝점을 구부려 링 형태로 하면 해시 링(hash ring)이 만들어진다.
- 이 해시 링을 기준으로 안정 해시에 대해 알아보자

![스크린샷 2022-10-03 오후 5.07.11.png](https://github.com/BOAZ-bigdata/24-1_Study_Large-ScaleSystemDesign/assets/72328687/094bd35a-b76f-46d1-8d20-f6e62f5c8db1)

### 해시 서버 조회

어떤 키가 저장되는 서버는 해당 키의 위치로부터 시계 방향으로 링을 탐색해 나가면서 만나는 첫 번째 서버가 된다

![스크린샷 2022-10-03 오후 5.09.54.png](https://github.com/BOAZ-bigdata/24-1_Study_Large-ScaleSystemDesign/assets/72328687/b0134596-940d-46cc-a328-2c88ddedc4ea)

### 서버 추가와 삭제

해시 링에서 서버를 추가할 때는 키 가운데 일부만 재배치하면 된다.

- key0 → server0 로 배치되어있던 구조에서
- key0 → server4 → server0 으로 변경된다면
- server0을 가리키던 key값만 server6으로 변경하면 된다.
- 삭제도 이와 유사하고 일부 키만 재배치한다.

![스크린샷 2022-10-03 오후 5.08.08.png](https://github.com/BOAZ-bigdata/24-1_Study_Large-ScaleSystemDesign/assets/72328687/e808cc82-03de-4258-b0e1-f0ddbe510514)

### 가상 노드를 활용한 균등 분포

지금까지 알아본 방식으로는 특정 서버는 굉장히 큰 해시 공간을 차지하거나 어떤 서버는 데이터를 아무것도 갖지 않는 경우도 생길 수 있어 균등하게 분포되지 않을 수 있는데, 이를 `가상 노드`(Virtual Node)를 사용해서 해결해볼 수 있다.

- `가상 노드(Virtual Node)`
  - 가상 노드는 실제 노드 또는 서버를 가리키는 노드(일종의 포인터)로 하나의 서버는 링 위에서 여러 개의 가상 노드를 가질 수 있다.
  - 가상 노드의 개수를 늘리면 데이터가 어떻게 퍼져있는지 나타내는 표준 편차(standard deviation)가 작아져서 키의 분포가 점점 더 균등해지게 된다.
  - 가상 노드를 늘릴 수록 표준 편차는 줄어들고 대신에 비용이 증가하므로 tradeoff를 잘 고려하여 적절히 조정하는 것이 중요하다.
  - 이를 이미지로 살펴보면 다음과 같다.

![스크린샷 2022-10-03 오후 5.14.45.png](https://github.com/BOAZ-bigdata/24-1_Study_Large-ScaleSystemDesign/assets/72328687/042107a8-96f8-445d-8903-6c67aadc8ca6)

## 요약

안정 해시의 이점을 정리해보면 다음과 같다.

- 서버가 추가되거나 삭제될 때 재배치되는 키의 수를 최소화 한다.
- 데이터가 보다 균등하게 분포되므로 수평적 규모 확장성을 달성하기 쉽다
- 핫스팟(hotspot) 키 문제를 줄인다.

## 안정 해시 사용 사례

- AWS DynamoDB의 파티셔닝 관련 컴포넌트
- Apache Cassandra 클러스터에서의 데이터 파티셔닝
- Discord 채팅 application
- 아카마이(Akamai) CDN
- 매그레프(Meglev) 네트워크 부하 분산기
